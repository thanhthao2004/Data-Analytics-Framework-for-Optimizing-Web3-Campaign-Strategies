# analysis/pillar2_gas_model.py (KH·ªöP V·ªöI S∆† ƒê·ªí)
import pandas as pd
import numpy as np
from connectors.db_connector import BigQueryConnector
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX  # PHASE 2: SARIMAX support
from statsmodels.tsa.stattools import adfuller
from sklearn.model_selection import TimeSeriesSplit  # PHASE 2: Cross-validation
import warnings

class GasCostForecaster:
    """
    Tri·ªÉn khai Tr·ª• c·ªôt 2: M√¥ h√¨nh Kinh t·∫ø Gas.
    Kh·ªõp v·ªõi s∆° ƒë·ªì Mermaid P2.
    
    M√¥ h√¨nh D·ª± b√°o: ARIMA(1, 1, 1)
    ===============================
    
    ƒê·ªô tin c·∫≠y v√† Gi·ªõi h·∫°n:
    ----------------------
    - M√¥ h√¨nh ARIMA(1, 1, 1) ph√π h·ª£p ƒë·ªÉ n·∫Øm b·∫Øt xu h∆∞·ªõng v√† t√≠nh m√πa v·ª• NG·∫ÆN H·∫†N 
      (d∆∞·ªõi 7 ng√†y) c·ªßa Base Fee tr√™n Ethereum.
    - ƒê·ªô ch√≠nh x√°c s·∫Ω GI·∫¢M NHANH khi k√©o d√†i d·ª± b√°o qu√° 7 ng√†y do:
      * Base Fee tr√™n Ethereum bi·∫øn ƒë·ªông ph·ª• thu·ªôc v√†o nhi·ªÅu y·∫øu t·ªë ph·ª©c t·∫°p
      * ARIMA l√† m√¥ h√¨nh tuy·∫øn t√≠nh, c√≥ th·ªÉ b·ªè l·ª° c√°c m·ªëi quan h·ªá phi tuy·∫øn
    
    ƒê·ªô nh·∫°y v√† C·∫£nh b√°o:
    --------------------
    - M√¥ h√¨nh R·∫§T NH·∫†Y C·∫¢M v·ªõi c√°c s·ª± ki·ªán kh√¥ng l∆∞·ªùng tr∆∞·ªõc (Black Swan events):
      * N√¢ng c·∫•p m·∫°ng (EIP-1559, Merge, c√°c hard fork kh√°c)
      * S·ª± ki·ªán mint NFT l·ªõn (v√≠ d·ª•: c√°c d·ª± √°n NFT h√†ng ƒë·∫ßu ph√°t h√†nh)
      * TƒÉng ƒë·ªôt bi·∫øn ph√≠ giao d·ªãch do c√° voi thao t√∫ng ho·∫∑c flash crash
      * C√°c s·ª± ki·ªán DeFi l·ªõn (liquidations h√†ng lo·∫°t, bridge hacks)
    - C·∫ßn c√≥ c∆° ch·∫ø c·∫£nh b√°o ho·∫∑c fallback cho nh·ªØng tr∆∞·ªùng h·ª£p n√†y.
    - D·ªØ li·ªáu l·ªãch s·ª≠ ch·ªâ d√πng 30 ng√†y g·∫ßn nh·∫•t ƒë·ªÉ c√¢n b·∫±ng gi·ªØa ƒë·ªô m·ªõi v√† chi ph√≠ truy v·∫•n.
    
    Khuy·∫øn ngh·ªã S·ª≠ d·ª•ng:
    -------------------
    - S·ª≠ d·ª•ng d·ª± b√°o cho c·ª≠a s·ªï 4 gi·ªù (rolling average) ƒë·ªÉ gi·∫£m nhi·ªÖu.
    - K·∫øt h·ª£p v·ªõi c√°c ngu·ªìn d·ªØ li·ªáu kh√°c (GasNow, Etherscan forecasters) ƒë·ªÉ x√°c nh·∫≠n.
    - Kh√¥ng n√™n ch·ªâ d·ª±a v√†o m√¥ h√¨nh n√†y cho c√°c quy·∫øt ƒë·ªãnh quan tr·ªçng v·ªÅ chi ph√≠.
    """
    def __init__(self, db: BigQueryConnector):
        self.db = db
        self.model_fit = None

    def _fetch_hourly_gas(self, days_back=30):
        """
        L·∫•y d·ªØ li·ªáu base_fee trung b√¨nh h√†ng gi·ªù t·ª´ BigQuery v·ªõi exogenous features.
        Kh·ªõp v·ªõi lu·ªìng: _fetch_hourly_gas() -> Run BigQuery SQL
        
        PHASE 2 UPGRADE: Th√™m exogenous variables cho SARIMAX:
        - network_utilization: AVG(gas_used) / AVG(gas_limit) - m·ª©c ƒë·ªô s·ª≠ d·ª•ng m·∫°ng
        - transaction_count: S·ªë l∆∞·ª£ng transactions m·ªói gi·ªù
        - day_of_week: 1-7 (Monday-Sunday)
        - hour_of_day: 0-23 (UTC)
        
        NOTE: base_fee_per_gas trong BigQuery ƒë√£ ·ªü ƒë∆°n v·ªã Wei (s·ªë nguy√™n).
        Chia cho 1e9 ƒë·ªÉ chuy·ªÉn sang Gwei.
        
        Returns:
            pandas.DataFrame v·ªõi columns: avg_gwei, network_utilization, 
            transaction_count, day_of_week, hour_of_day
            ho·∫∑c None n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu
        """
        print(f"[Pillar 2] ƒêang l·∫•y d·ªØ li·ªáu gas l·ªãch s·ª≠ ({days_back} ng√†y) v·ªõi exogenous features...")
        
        # PHASE 2: Enhanced query v·ªõi exogenous variables
        query = f"""
            SELECT
                hour,
                avg_gwei,
                network_utilization,
                transaction_count,
                -- Temporal features: Extract from already-grouped hour
                EXTRACT(DAYOFWEEK FROM hour) AS day_of_week,  -- 1=Sunday, 7=Saturday
                EXTRACT(HOUR FROM hour) AS hour_of_day
            FROM (
                SELECT
                    TIMESTAMP_TRUNC(timestamp, HOUR) AS hour,
                    AVG(base_fee_per_gas) / 1e9 AS avg_gwei,
                    AVG(gas_used) / NULLIF(AVG(gas_limit), 0) AS network_utilization,
                    COUNT(*) AS transaction_count
                FROM `bigquery-public-data.crypto_ethereum.blocks`
                WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {days_back} DAY)
                  AND base_fee_per_gas IS NOT NULL
                  AND base_fee_per_gas > 0
                  AND gas_used IS NOT NULL
                  AND gas_limit > 0
                GROUP BY 1
            )
            ORDER BY hour
        """
        df = self.db.query_to_dataframe(query)
        if df.empty:
            print("[Pillar 2] Kh√¥ng c√≥ d·ªØ li·ªáu gas.")
            return None
        
        df['hour'] = pd.to_datetime(df['hour'], utc=True)
        df.set_index('hour', inplace=True)
        
        # PHASE 2: Ensure correct dtypes for SARIMAX (critical fix)
        # MUST use float64 for all columns - statsmodels doesn't support Int64
        df['avg_gwei'] = pd.to_numeric(df['avg_gwei'], errors='coerce').astype('float64')
        df['network_utilization'] = pd.to_numeric(df['network_utilization'], errors='coerce').astype('float64')
        df['transaction_count'] = pd.to_numeric(df['transaction_count'], errors='coerce').astype('float64')
        df['day_of_week'] = pd.to_numeric(df['day_of_week'], errors='coerce').astype('float64')  # float, not Int64!
        df['hour_of_day'] = pd.to_numeric(df['hour_of_day'], errors='coerce').astype('float64')  # float, not Int64!
        
        # Resample to hourly frequency
        df = df.resample('h').ffill()
        
        # Fill remaining NULLs
        null_counts = df.isnull().sum()
        if null_counts.any():
            print(f"[Pillar 2] ‚ö†Ô∏è  Filling {null_counts.sum()} NULL values")
            df = df.fillna(df.mean())
        
        # Validation
        min_gas = df['avg_gwei'].min()
        max_gas = df['avg_gwei'].max()
        mean_gas = df['avg_gwei'].mean()
        print(f"[Pillar 2] D·ªØ li·ªáu gas: Min={min_gas:.4f} Gwei, Max={max_gas:.4f} Gwei, Mean={mean_gas:.4f} Gwei")
        print(f"[Pillar 2] Network utilization: Mean={df['network_utilization'].mean():.3f}")
        print(f"[Pillar 2] Transaction count: Mean={df['transaction_count'].mean():.0f}")
        
        return df


    def _train_model(self, endog_data, exog_data=None):
        """
        Hu·∫•n luy·ªán m√¥ h√¨nh SARIMAX (PHASE 2 UPGRADE t·ª´ ARIMA).
        
        SARIMAX = Seasonal Auto-Regressive Integrated Moving Average with eXogenous variables
        - Seasonal order (P,D,Q,s): (1,1,1,24) - chu k·ª≥ 24 gi·ªù
        - Exogenous variables: network_utilization, day_of_week, hour_of_day, etc.
        
        Args:
            endog_data: pandas.Series - Target variable (avg_gwei)
            exog_data: pandas.DataFrame - Exogenous variables (optional)
        """
        print("[Pillar 2] ƒêang hu·∫•n luy·ªán m√¥ h√¨nh SARIMAX...")
        warnings.filterwarnings("ignore")  # T·∫Øt c·∫£nh b√°o statsmodels
        
        try:
            # PHASE 2: SARIMAX with seasonal component (24-hour cycle)
            # order=(1,1,1): ARIMA parameters (p,d,q)
            # seasonal_order=(1,1,1,24): Seasonal ARIMA parameters (P,D,Q,s)
            #   s=24: 24-hour cycle (daily seasonality)
            model = SARIMAX(
                endog_data,
                exog=exog_data,
                order=(1, 1, 1),
                seasonal_order=(1, 1, 1, 24),
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            self.model_fit = model.fit(disp=False, maxiter=200)
            print("[Pillar 2] Hu·∫•n luy·ªán m√¥ h√¨nh SARIMAX ho√†n t·∫•t.")
            
        except Exception as e:
            print(f"[Pillar 2] ‚ö†Ô∏è  C·∫¢NH B√ÅO: SARIMAX training failed: {e}")
            print("[Pillar 2] Fallback to simple ARIMA model...")
            # Fallback to ARIMA if SARIMAX fails
            model = ARIMA(endog_data, order=(1, 1, 1))
            self.model_fit = model.fit()
            print("[Pillar 2] ƒê√£ fallback sang ARIMA th√†nh c√¥ng.")
        
        warnings.filterwarnings("default")


    def _cross_validate_model(self, endog_data, exog_data=None, n_splits=5):
        """
        PHASE 2: Cross-validation cho SARIMAX model.
        
        S·ª≠ d·ª•ng TimeSeriesSplit ƒë·ªÉ validate model tr√™n nhi·ªÅu folds.
        ƒê√¢y l√† ph∆∞∆°ng ph√°p robust h∆°n single train-test split.
        
        Args:
            endog_data: pandas.Series - Target variable (avg_gwei)
            exog_data: pandas.DataFrame - Exogenous variables
            n_splits: int - S·ªë l∆∞·ª£ng folds (default: 5)
        
        Returns:
            dict: Aggregated metrics v·ªõi mean v√† std across folds
        """
        if len(endog_data) < 120:  # C·∫ßn √≠t nh·∫•t 120 gi·ªù (5 ng√†y) cho CV
            print("[Pillar 2] Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho cross-validation (c·∫ßn >= 120 gi·ªù).")
            return None
        
        print(f"[Pillar 2] ƒêang ch·∫°y {n_splits}-fold cross-validation v·ªõi SARIMAX...")
        
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        fold_metrics = {
            'mae': [],
            'rmse': [],
            'mape': [],
            'r_squared': []
        }
        
        warnings.filterwarnings("ignore")
        
        for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(endog_data)):
            try:
                # Split data
                y_train = endog_data.iloc[train_idx]
                y_test = endog_data.iloc[test_idx]
                
                X_train = exog_data.iloc[train_idx] if exog_data is not None else None
                X_test = exog_data.iloc[test_idx] if exog_data is not None else None
                
                # Train SARIMAX on this fold
                model = SARIMAX(
                    y_train,
                    exog=X_train,
                    order=(1, 1, 1),
                    seasonal_order=(1, 1, 1, 24),
                    enforce_stationarity=False,
                    enforce_invertibility=False
                )
                model_fit = model.fit(disp=False, maxiter=100)
                
                # Forecast on test set
                if X_test is not None:
                    forecast = model_fit.get_forecast(steps=len(y_test), exog=X_test)
                else:
                    forecast = model_fit.get_forecast(steps=len(y_test))
                
                y_pred = forecast.predicted_mean
                y_true = y_test.values
                
                # Calculate metrics for this fold
                mask = ~(np.isnan(y_pred) | np.isnan(y_true))
                if mask.sum() > 0:
                    y_pred_clean = y_pred[mask]
                    y_true_clean = y_true[mask]
                    
                    mae = np.mean(np.abs(y_pred_clean - y_true_clean))
                    rmse = np.sqrt(np.mean((y_pred_clean - y_true_clean) ** 2))
                    
                    # MAPE
                    non_zero = y_true_clean != 0
                    if non_zero.sum() > 0:
                        mape = np.mean(np.abs((y_true_clean[non_zero] - y_pred_clean[non_zero]) / 
                                             y_true_clean[non_zero])) * 100
                    else:
                        mape = np.nan
                    
                    # R¬≤
                    ss_res = np.sum((y_true_clean - y_pred_clean) ** 2)
                    ss_tot = np.sum((y_true_clean - np.mean(y_true_clean)) ** 2)
                    r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else np.nan
                    
                    fold_metrics['mae'].append(mae)
                    fold_metrics['rmse'].append(rmse)
                    if not np.isnan(mape):
                        fold_metrics['mape'].append(mape)
                    if not np.isnan(r_squared):
                        fold_metrics['r_squared'].append(r_squared)
                    
                    print(f"  Fold {fold_idx+1}/{n_splits}: MAE={mae:.4f}, RMSE={rmse:.4f}, "
                          f"MAPE={mape:.2f}%, R¬≤={r_squared:.4f}")
                
            except Exception as e:
                print(f"  Fold {fold_idx+1}/{n_splits}: Failed - {str(e)[:50]}")
                continue
        
        warnings.filterwarnings("default")
        
        # Aggregate results
        if len(fold_metrics['mae']) == 0:
            print("[Pillar 2] Cross-validation failed on all folds.")
            return None
        
        cv_results = {
            'mae_mean': np.mean(fold_metrics['mae']),
            'mae_std': np.std(fold_metrics['mae']),
            'rmse_mean': np.mean(fold_metrics['rmse']),
            'rmse_std': np.std(fold_metrics['rmse']),
            'mape_mean': np.mean(fold_metrics['mape']) if fold_metrics['mape'] else np.nan,
            'mape_std': np.std(fold_metrics['mape']) if fold_metrics['mape'] else np.nan,
            'r_squared_mean': np.mean(fold_metrics['r_squared']) if fold_metrics['r_squared'] else np.nan,
            'r_squared_std': np.std(fold_metrics['r_squared']) if fold_metrics['r_squared'] else np.nan,
            'n_successful_folds': len(fold_metrics['mae']),
            'n_total_folds': n_splits
        }
        
        print(f"\n[Pillar 2] Cross-validation results ({cv_results['n_successful_folds']}/{n_splits} folds):")
        print(f"  MAE: {cv_results['mae_mean']:.4f} ¬± {cv_results['mae_std']:.4f} Gwei")
        print(f"  RMSE: {cv_results['rmse_mean']:.4f} ¬± {cv_results['rmse_std']:.4f} Gwei")
        if not np.isnan(cv_results['mape_mean']):
            print(f"  MAPE: {cv_results['mape_mean']:.2f} ¬± {cv_results['mape_std']:.2f}%")
        if not np.isnan(cv_results['r_squared_mean']):
            print(f"  R¬≤: {cv_results['r_squared_mean']:.4f} ¬± {cv_results['r_squared_std']:.4f}")
        
        return cv_results

    def _calculate_model_accuracy(self, data, exog_data=None):
        """
        PHASE 2 UPGRADE: T√≠nh to√°n ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh SARIMAX.
        
        Ph∆∞∆°ng ph√°p:
        1. ∆Øu ti√™n: Cross-validation (5-fold) n·∫øu ƒë·ªß d·ªØ li·ªáu (>= 120 gi·ªù)
        2. Fallback: Single train-test split (80/20) n·∫øu √≠t d·ªØ li·ªáu h∆°n
        
        Args:
            data: pandas.Series ho·∫∑c pandas.DataFrame - Target variable (avg_gwei)
            exog_data: pandas.DataFrame - Exogenous variables (optional)
        
        Returns:
            dict: C√°c metrics ƒë·ªô ch√≠nh x√°c
        """
        # Extract target variable if data is DataFrame
        if isinstance(data, pd.DataFrame):
            endog_data = data['avg_gwei'] if 'avg_gwei' in data.columns else data.iloc[:, 0]
        else:
            endog_data = data
        
        # PHASE 2: Prefer cross-validation if enough data
        if len(endog_data) >= 120:
            cv_results = self._cross_validate_model(endog_data, exog_data, n_splits=5)
            if cv_results is not None:
                # Convert CV results to standard format
                return {
                    'mae': cv_results['mae_mean'],
                    'mae_std': cv_results['mae_std'],
                    'rmse': cv_results['rmse_mean'],
                    'rmse_std': cv_results['rmse_std'],
                    'mape': cv_results['mape_mean'],
                    'mape_std': cv_results['mape_std'],
                    'r_squared': cv_results['r_squared_mean'],
                    'r_squared_std': cv_results['r_squared_std'],
                    'validation_method': 'cross_validation',
                    'n_folds': cv_results['n_successful_folds']
                }
        
        # Fallback to single train-test split
        if len(endog_data) < 48:
            print("[Pillar 2] Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c (c·∫ßn >= 48 gi·ªù).")
            return None
        
        print("[Pillar 2] S·ª≠ d·ª•ng single train-test split (80/20)...")
        
        # Chia d·ªØ li·ªáu: 80% train, 20% test
        split_idx = int(len(endog_data) * 0.8)
        y_train = endog_data[:split_idx]
        y_test = endog_data[split_idx:]
        
        X_train = exog_data[:split_idx] if exog_data is not None else None
        X_test = exog_data[split_idx:] if exog_data is not None else None
        
        print(f"[Pillar 2] ƒêang ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c: Train={len(y_train)}h, Test={len(y_test)}h...")
        
        warnings.filterwarnings("ignore")
        try:
            # Train SARIMAX model
            model_val = SARIMAX(
                y_train,
                exog=X_train,
                order=(1, 1, 1),
                seasonal_order=(1, 1, 1, 24),
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            model_fit_val = model_val.fit(disp=False, maxiter=100)
            
            # Forecast on test set
            if X_test is not None:
                forecast_test = model_fit_val.get_forecast(steps=len(y_test), exog=X_test)
            else:
                forecast_test = model_fit_val.get_forecast(steps=len(y_test))
            
            predicted_values = forecast_test.predicted_mean
            actual_values = y_test.values
            
            # Lo·∫°i b·ªè NaN n·∫øu c√≥
            mask = ~(np.isnan(predicted_values) | np.isnan(actual_values))
            if mask.sum() == 0:
                print("[Pillar 2] Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ t√≠nh to√°n ƒë·ªô ch√≠nh x√°c.")
                return None
                
            predicted_clean = predicted_values[mask]
            actual_clean = actual_values[mask]
            
            # T√≠nh c√°c metrics
            mae = np.mean(np.abs(predicted_clean - actual_clean))
            rmse = np.sqrt(np.mean((predicted_clean - actual_clean) ** 2))
            
            # MAPE: Tr√°nh chia cho 0
            non_zero_mask = actual_clean != 0
            if non_zero_mask.sum() > 0:
                mape = np.mean(np.abs((actual_clean[non_zero_mask] - predicted_clean[non_zero_mask]) / actual_clean[non_zero_mask])) * 100
            else:
                mape = np.nan
            
            # T√≠nh R-squared (coefficient of determination)
            ss_res = np.sum((actual_clean - predicted_clean) ** 2)
            ss_tot = np.sum((actual_clean - np.mean(actual_clean)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else np.nan
            
            # Model fit metrics t·ª´ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
            aic = model_fit_val.aic
            bic = model_fit_val.bic
            log_likelihood = model_fit_val.llf
            
            accuracy_metrics = {
                "mae": mae,
                "rmse": rmse,
                "mape": mape,
                "r_squared": r_squared,
                "aic": aic,
                "bic": bic,
                "log_likelihood": log_likelihood,
                "test_samples": len(test_data)
            }
            
            warnings.filterwarnings("default")
            return accuracy_metrics
            
        except Exception as e:
            print(f"[Pillar 2] L·ªói khi ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c: {e}")
            warnings.filterwarnings("default")
            return None

    def run(self, forecast_days=7, use_cache: bool = False, save_cache: bool = True) -> dict:
        """
        Ch·∫°y ph√¢n t√≠ch Pillar 2 ƒë·∫ßy ƒë·ªß.
        Kh·ªõp v·ªõi lu·ªìng: Forecast -> Compute rolling(4h).mean() -> Find min avg_gwei
        
        Args:
            forecast_days: S·ªë ng√†y d·ª± b√°o (m·∫∑c ƒë·ªãnh: 7)
            use_cache: N·∫øu True, s·∫Ω ƒë·ªçc t·ª´ cache n·∫øu c√≥, kh√¥ng query l·∫°i BigQuery
            save_cache: N·∫øu True, s·∫Ω l∆∞u k·∫øt qu·∫£ v√†o cache sau khi ph√¢n t√≠ch
        """
        # Import DataCache ·ªü ƒë√¢y ƒë·ªÉ tr√°nh circular import
        from analysis.data_cache import DataCache
        
        cache = DataCache()
        
        # Th·ª≠ ƒë·ªçc t·ª´ cache n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if use_cache:
            cached_result = cache.load_pillar2(forecast_days)
            if cached_result is not None:
                print("[Pillar 2] ƒê√£ s·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ cache (kh√¥ng query BigQuery).")
                return cached_result
        
        print("\n--- B·∫Øt ƒë·∫ßu Ph√¢n t√≠ch Pillar 2: Chi ph√≠ Gas ---")
        
        # Th·ª≠ ƒë·ªçc d·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ cache tr∆∞·ªõc
        data = None
        if use_cache:
            data = cache.load_pillar2_historical(days_back=30)
        
        # N·∫øu kh√¥ng c√≥ trong cache, query t·ª´ BigQuery
        if data is None:
            data = self._fetch_hourly_gas(days_back=30)
            # L∆∞u d·ªØ li·ªáu l·ªãch s·ª≠ v√†o cache
            if save_cache and data is not None:
                cache.save_pillar2_historical(data, days_back=30)
        
        if data is None:
            return {"error": "Kh√¥ng c√≥ d·ªØ li·ªáu gas"}
        
        # PHASE 2: Extract target and exogenous variables
        if isinstance(data, pd.DataFrame):
            endog_data = data['avg_gwei']
            # Exogenous features (all columns except avg_gwei)
            exog_cols = [col for col in data.columns if col != 'avg_gwei']
            exog_data = data[exog_cols] if exog_cols else None
        else:
            # Backward compatibility: if data is Series (old ARIMA cache)
            endog_data = data
            exog_data = None
        
        # Train model with exogenous variables
        self._train_model(endog_data, exog_data)
        
        # PHASE 2: T√≠nh to√°n ƒë·ªô ch√≠nh x√°c v·ªõi cross-validation
        accuracy_metrics = self._calculate_model_accuracy(endog_data, exog_data)
        
        # In ra c√°c metrics ƒë·ªô ch√≠nh x√°c
        if accuracy_metrics:
            print("\n=== ƒê·ªò CH√çNH X√ÅC M√î H√åNH ARIMA (Backtesting) ===")
            print(f"  ‚Ä¢ MAE (Mean Absolute Error): {accuracy_metrics['mae']:.4f} Gwei")
            print(f"  ‚Ä¢ RMSE (Root Mean Squared Error): {accuracy_metrics['rmse']:.4f} Gwei")
            if not np.isnan(accuracy_metrics['mape']):
                print(f"  ‚Ä¢ MAPE (Mean Absolute Percentage Error): {accuracy_metrics['mape']:.2f}%")
            if not np.isnan(accuracy_metrics['r_squared']):
                print(f"  ‚Ä¢ R¬≤ (Coefficient of Determination): {accuracy_metrics['r_squared']:.4f}")
                print(f"    ‚Üí Gi·∫£i th√≠ch: {accuracy_metrics['r_squared']*100:.2f}% ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi m√¥ h√¨nh")
            
            # AIC/BIC only available from single model fit, not cross-validation
            if 'aic' in accuracy_metrics:
                print(f"  ‚Ä¢ AIC (Akaike Information Criterion): {accuracy_metrics['aic']:.2f}")
            if 'bic' in accuracy_metrics:
                print(f"  ‚Ä¢ BIC (Bayesian Information Criterion): {accuracy_metrics['bic']:.2f}")
            
            # Validation method
            validation_method = accuracy_metrics.get('validation_method', 'unknown')
            print(f"\nüìä Validation Method: {validation_method}")
            
            if 'log_likelihood' in accuracy_metrics: # Added conditional check for log_likelihood
                print(f"  ‚Ä¢ Log Likelihood: {accuracy_metrics['log_likelihood']:.2f}")
            if 'test_samples' in accuracy_metrics: # Added conditional check for test_samples
                print(f"  ‚Ä¢ S·ªë m·∫´u test: {accuracy_metrics['test_samples']} gi·ªù")
            
            # ƒê√°nh gi√° ƒë·ªô tin c·∫≠y d·ª±a tr√™n MAPE v√† R¬≤
            mape = accuracy_metrics.get('mape', np.nan)
            r_squared = accuracy_metrics.get('r_squared', np.nan)
            
            if not np.isnan(mape):
                if mape < 5:
                    reliability = "R·∫§T CAO"
                    reliability_color = "GREEN"
                elif mape < 10:
                    reliability = "CAO"
                    reliability_color = "GREEN"
                elif mape < 20:
                    reliability = "TRUNG B√åNH"
                    reliability_color = "YELLOW"
                elif mape < 50:
                    reliability = "TH·∫§P"
                    reliability_color = "ORANGE"
                else:
                    reliability = "KH√îNG ƒê√ÅNG TIN C·∫¨Y"
                    reliability_color = "RED"
                print(f"\n  üìä ƒê·ªò TIN C·∫¨Y D·ª∞ B√ÅO: {reliability} (MAPE = {mape:.2f}%)")
            
            # C·∫¢NH B√ÅO NGHI√äM TR·ªåNG n·∫øu R¬≤ < 0 ho·∫∑c MAPE > 100%
            if not np.isnan(r_squared) and r_squared < 0:
                print(f"\n  ‚ö†Ô∏è  C·∫¢NH B√ÅO NGHI√äM TR·ªåNG: R¬≤ = {r_squared:.6f} < 0")
                print(f"     ‚Üí M√¥ h√¨nh d·ª± b√°o T·ªÜ H∆†N c·∫£ vi·ªác d·ª± ƒëo√°n gi√° tr·ªã trung b√¨nh!")
                print(f"     ‚Üí D·ª± b√°o gas KH√îNG ƒê√ÅNG TIN C·∫¨Y. N√™n s·ª≠ d·ª•ng gi·ªù peak user (P3) thay v√¨ gas window (P2).")
            
            if not np.isnan(mape) and mape > 100:
                print(f"\n  ‚ö†Ô∏è  C·∫¢NH B√ÅO NGHI√äM TR·ªåNG: MAPE = {mape:.2f}% > 100%")
                print(f"     ‚Üí M√¥ h√¨nh ARIMA KH√îNG PH√ô H·ª¢P v·ªõi d·ªØ li·ªáu gas hi·ªán t·∫°i.")
                print(f"     ‚Üí Nguy√™n nh√¢n c√≥ th·ªÉ: D·ªØ li·ªáu nhi·ªÖu cao, m√¥ h√¨nh tuy·∫øn t√≠nh kh√¥ng n·∫Øm b·∫Øt ƒë∆∞·ª£c t√≠nh phi tuy·∫øn.")
                print(f"     ‚Üí KHUY·∫æN NGH·ªä: B·ªè qua d·ª± b√°o gas (P2), ch·ªâ d·ª±a v√†o gi·ªù peak user (P3) ƒë·ªÉ quy·∫øt ƒë·ªãnh.")
            
            print("=" * 50)
        else:
            print("[Pillar 2] Kh√¥ng th·ªÉ ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c (thi·∫øu d·ªØ li·ªáu ho·∫∑c l·ªói).")
        
        # L·∫•y c√°c metrics t·ª´ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán (full data)
        if self.model_fit is not None:
            full_model_metrics = {
                "aic_full": self.model_fit.aic,
                "bic_full": self.model_fit.bic,
                "log_likelihood_full": self.model_fit.llf
            }
        else:
            full_model_metrics = {}
        
        steps_to_forecast = forecast_days * 24  # 7 ng√†y * 24 gi·ªù
        print(f"\n[Pillar 2] ƒêang d·ª± b√°o cho {steps_to_forecast} gi·ªù t·ªõi...")
        
        # PHASE 2: Generate future exogenous features for forecast period
        if exog_data is not None:
            # Get last timestamp from data
            last_time = endog_data.index[-1]
            
            # Generate future timestamps
            future_times = pd.date_range(start=last_time + pd.Timedelta(hours=1), 
                                        periods=steps_to_forecast, freq='H')
            
            # Create future exogenous DataFrame
            future_exog = pd.DataFrame(index=future_times)
            
            # day_of_week: Extract from timestamp (1=Sunday, 7=Saturday)
            if 'day_of_week' in exog_cols:
                future_exog['day_of_week'] = future_times.dayofweek + 2  # Convert to 1-7
                future_exog['day_of_week'] = future_exog['day_of_week'].replace(8, 1)  # Sunday
            
            # hour_of_day: Extract from timestamp (0-23)
            if 'hour_of_day' in exog_cols:
                future_exog['hour_of_day'] = future_times.hour
            
            # network_utilization: Use historical mean (assumption)
            if 'network_utilization' in exog_cols:
                future_exog['network_utilization'] = exog_data['network_utilization'].mean()
            
            # transaction_count: Use historical mean (assumption)
            if 'transaction_count' in exog_cols:
                future_exog['transaction_count'] = exog_data['transaction_count'].mean()
            
            # Reorder columns to match training data
            future_exog = future_exog[exog_cols]
            
            # Forecast with exogenous variables
            forecast = self.model_fit.get_forecast(steps=steps_to_forecast, exog=future_exog.values)
        else:
            # Fallback: forecast without exog (old ARIMA behavior)
            forecast = self.model_fit.get_forecast(steps=steps_to_forecast)
        
        forecast_df = forecast.conf_int(alpha=0.05)
        forecast_df['predicted_gwei'] = forecast.predicted_mean
        
        # Kh·ªõp v·ªõi lu·ªìng: Compute rolling(4h).mean()
        window_size_hours = 4
        rolling_avg = forecast_df['predicted_gwei'].rolling(window=window_size_hours).mean()
        
        # Kh·ªõp v·ªõi lu·ªìng: Find min avg_gwei -> best_window_start
        best_window_start = rolling_avg.idxmin()
        if pd.isna(best_window_start):
             best_window_start = forecast_df['predicted_gwei'].idxmin()
             
        best_gas = rolling_avg.min()
        
        print(f"[Pillar 2] Ho√†n t·∫•t. C·ª≠a s·ªï 4 gi·ªù r·∫ª nh·∫•t b·∫Øt ƒë·∫ßu l√∫c: {best_window_start} UTC")
        
        result = {
            "best_window_start_utc": str(best_window_start),
            "estimated_avg_gwei": best_gas,
            "forecast_dataframe": forecast_df
        }
        
        # Th√™m accuracy metrics v√†o k·∫øt qu·∫£
        if accuracy_metrics:
            result["model_accuracy"] = accuracy_metrics
        
        if full_model_metrics:
            result["model_fit_metrics"] = full_model_metrics
        
        # L∆∞u v√†o cache n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if save_cache:
            cache.save_pillar2(result, forecast_days)
        
        return result